<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Model validation and comparison • mbg</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<meta property="og:title" content="Model validation and comparison">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">mbg</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.9.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../index.html">Home</a></li>
<li class="nav-item"><a class="nav-link" href="../articles/mbg.html">Getting started</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-more-tutorials" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">More tutorials</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-more-tutorials">
<li><a class="dropdown-item" href="../articles/spatial-ml-models.html">Running spatial ML models</a></li>
    <li><a class="dropdown-item" href="../articles/all-model-options.html">All MBG model runner options</a></li>
    <li><a class="dropdown-item" href="../articles/model-comparison.html">Model validation and comparison</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Function reference</a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Model validation and comparison</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/henryspatialanalysis/mbg/blob/main/vignettes/model-comparison.Rmd" class="external-link"><code>vignettes/model-comparison.Rmd</code></a></small>
      <div class="d-none name"><code>model-comparison.Rmd</code></div>
    </div>

    
    
<p>The <code>mbg</code> package allows for easy customization of
geostatistical models: for example, it is simple to test models with
varying covariate sets, approaches for relating covariates to the
outcome, and combinations of model effects. This leads to the obvious
question of which model is best for the situation at hand. In this
article, we explore how to compare models using standard predictive
validity metrics and k-fold cross-validation.</p>
<div class="section level2">
<h2 id="predictive-validity-metrics">Predictive validity metrics<a class="anchor" aria-label="anchor" href="#predictive-validity-metrics"></a>
</h2>
<p>In this tutorial, we will generate the following metrics to assess
and compare models:</p>
<div class="section level3">
<h3 id="log-predictive-density">Log predictive density<a class="anchor" aria-label="anchor" href="#log-predictive-density"></a>
</h3>
<p>In a Bayesian context, we would ideally like to evaluate a posterior
predictive distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>i</mi><mi>o</mi><mi>r</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p_{posterior}(\theta)</annotation></semantics></math>
against some new data
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>y</mi><mo accent="true">̃</mo></mover><annotation encoding="application/x-tex">\tilde{y}</annotation></semantics></math>
drawn from the true underlying distribution. If we had this
newly-observed data, we could calculate the probability of observing
each new point given the predictive distribution, then multiply these
across <em>N</em> observations to get the overall <em>predictive density
(PD)</em>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>D</mi><mo>=</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>y</mi><mo accent="true">̃</mo></mover><mo stretchy="false" form="prefix">|</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̃</mo></mover><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
PD = p(\tilde{y}|\theta) = \prod_{i=1}^{N} p(\tilde{y}_i | \theta)
</annotation></semantics></math></p>
<p>When comparing two models against the same new data, the model with
the higher predictive density can be considered more consistent with
that data.</p>
<p>To make the computation more tractable, we can take the log of both
sides to calculate <em>log predictive density (LPD)</em>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>P</mi><mi>D</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mo>log</mo><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̃</mo></mover><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">
LPD = \sum_{i=1}^N \log{ p(\tilde{y}_i | \theta) }
</annotation></semantics></math></p>
<p>Because we are working with posterior samples rather than the full
distribution, we will average density across the set of predictive
samples
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
to approximate LPD:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mrow><mi>L</mi><mi>P</mi><mi>D</mi></mrow><mo accent="true">̂</mo></mover><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mn>1</mn><mi>S</mi></mfrac><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̃</mo></mover><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>θ</mi><mi>s</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\widehat{LPD} = \sum_{i=1}^{N} \log \left( \frac{1}{S} \sum_{s=1}^{S} p(\tilde{y}_i | \theta_s) \right)
</annotation></semantics></math></p>
<p>In the notation above, we are repeatedly evaluating each data point
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
(for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
= 1 to <em>N</em>) against each predictive posterior draw <em>s</em>
(for <em>s</em> = 1 to <em>S</em>).</p>
<p>The LPD is expressed as a negative number—smaller negative numbers
(those closer to zero) indicate a higher predictive density and
therefore a better predictive fit.</p>
</div>
<div class="section level3">
<h3 id="watanabe-aikake-information-criterion">Watanabe-Aikake information criterion<a class="anchor" aria-label="anchor" href="#watanabe-aikake-information-criterion"></a>
</h3>
<p>The <a href="https://en.wikipedia.org/wiki/Watanabe%E2%80%93Akaike_information_criterion" class="external-link">Watanabe-Aikake
information criterion</a> (WAIC, also called the “widely applicable
information criterion”) estimates how well a Bayesian model might fit to
new data without actually performing cross-validation. It is calculated
as the log pointwise predictive density (LPD, for data used to train the
model) and penalized by a term that is larger for more flexible
models.</p>
<p>Like other information criteria, WAIC is expressed as a positive
number, where smaller numbers indicate a better fit.</p>
</div>
<div class="section level3">
<h3 id="root-mean-squared-error">Root mean squared error<a class="anchor" aria-label="anchor" href="#root-mean-squared-error"></a>
</h3>
<p>Root mean squared error will be familiar to anyone who has taken an
introductory statistics course:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mi>M</mi><mi>S</mi><mi>E</mi><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow><annotation encoding="application/x-tex">
RMSE = \sqrt{ \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i )^{2} }
</annotation></semantics></math></p>
<p>In this context,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>
is the observed <em>rate</em> of the outcome (calculated as the number
of positives divided by the sample size) at data point
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
while
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\hat{y}_i</annotation></semantics></math>
is the model’s <em>mean prediction</em> at the pixel that overlaps with
data point
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>.
RMSE can be calculated “in-sample” against data that was used to fit the
model, or “out-of-sample” against data that was held back from the
model.</p>
<p>RMSE is simple to interpret; it can also be used as a point of
comparison against frequentist estimates like those generated by the
machine learning submodels. However, using RMSE to evalulate
geostatistical models has downsides:</p>
<ul>
<li>Differences in data sample size are ignored: an observation of 1
positive/4 sampled is identical to an observation of 250 positive/1,000
sampled when calculating
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>.</li>
<li>The uncertainty of model estimates is not taken into account: two
models with the same mean estimates but different uncertainty intervals
would end up with the same
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\hat{y}_i</annotation></semantics></math>
values and therefore identical RMSE.</li>
</ul>
<p>For these reasons, we prefer to use the LPD or WAIC for model
evaluation, and to use RMSE either as a backup or a tool for
specifically evaluating the model’s mean estimates.</p>
</div>
</div>
<div class="section level2">
<h2 id="model-cross-validation">Model cross-validation<a class="anchor" aria-label="anchor" href="#model-cross-validation"></a>
</h2>
<p>When calculating predictive validity metrics like LPD and RMSE, we
have a tension between (1) wanting to use all available data to fit the
model and (2) wanting to hold back some data for model validation. We
can deal with this tension using cross-validation.</p>
<p>In <strong>leave one out cross validation</strong>, we evaluate each
data point
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>
against a model that was trained on the entire dataset <em>except</em>
for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>;
we calculate each model’s predictive validity against the one unobserved
data point, then summarize those predictive validity metrics (summing
LPD and RMSE) across all subset models. In the aggregate, the
performance of each subset model against held-out data points should be
similar to the full model’s predictive performance against a theoretical
unobserved dataset drawn from the same distribution.</p>
<p>Running one model per data point is often too time-intensive to be
practical. We can approximate the performance of leave-one-out
cross-validation using <strong>k-folds cross-validation</strong>, where
the data is randomly split into <em>k</em> (often <em>k</em> = 5 or 10)
“folds” or holdouts. <em>k</em> subset models are run: in each, the fold
is reserved as a validation set, and the rest of the data is used to
train the subset. Predictive validity metrics are repeatedly calculated
for the held-out observations and then combined. When <em>k</em> is
large, the combined metrics will approach the values observed in
leave-one-out cross-validation.</p>
<p>In this tutorial, we will calculate all three of our standard metrics
for the full model; we will then calculate “out-of-sample” LPD and RMSE
using 10-fold cross validation.</p>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<p>In this tutorial, we will continue to use example data on child
stunting from Benin. We will compare the default <code>mbg</code> model,
as described in the <a href="mbg.html">introductory vignette</a>, to a
stacked ensemble model with department-level fixed effects, as described
in the <a href="model_comparison.html">spatial ML models
article</a>.</p>
<p>Start this tutorial by loading the example data and preparing the ID
raster:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Load packages</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://r-datatable.com" class="external-link">data.table</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://r-spatial.github.io/sf/" class="external-link">sf</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://rspatial.org/" class="external-link">terra</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://henryspatialanalysis.github.io/mbg/">mbg</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Load input data, covariates, and department boundaries</span></span>
<span><span class="va">outcomes</span> <span class="op">&lt;-</span> <span class="fu">data.table</span><span class="fu">::</span><span class="fu"><a href="https://rdatatable.gitlab.io/data.table/reference/fread.html" class="external-link">fread</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span><span class="st">'extdata/child_stunting.csv'</span>, package <span class="op">=</span> <span class="st">'mbg'</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">covariates</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  access <span class="op">=</span> <span class="fu">terra</span><span class="fu">::</span><span class="fu"><a href="https://rspatial.github.io/terra/reference/rast.html" class="external-link">rast</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span><span class="st">'extdata/access.tif'</span>, package <span class="op">=</span> <span class="st">'mbg'</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  evi <span class="op">=</span> <span class="fu">terra</span><span class="fu">::</span><span class="fu"><a href="https://rspatial.github.io/terra/reference/rast.html" class="external-link">rast</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span><span class="st">'extdata/evi.tif'</span>, package <span class="op">=</span> <span class="st">'mbg'</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  temperature <span class="op">=</span> <span class="fu">terra</span><span class="fu">::</span><span class="fu"><a href="https://rspatial.github.io/terra/reference/rast.html" class="external-link">rast</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span><span class="st">'extdata/temperature.tif'</span>, package <span class="op">=</span> <span class="st">'mbg'</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">covariates</span><span class="op">$</span><span class="va">intercept</span> <span class="op">&lt;-</span> <span class="va">covariates</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span> <span class="op">*</span> <span class="fl">0</span> <span class="op">+</span> <span class="fl">1</span></span>
<span><span class="va">departments</span> <span class="op">&lt;-</span> <span class="fu">sf</span><span class="fu">::</span><span class="fu"><a href="https://r-spatial.github.io/sf/reference/st_read.html" class="external-link">st_read</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/system.file.html" class="external-link">system.file</a></span><span class="op">(</span><span class="st">'extdata/Benin_departments.gpkg'</span>, package <span class="op">=</span> <span class="st">'mbg'</span><span class="op">)</span>,</span>
<span>  quiet <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Create ID raster</span></span>
<span><span class="va">id_raster</span> <span class="op">&lt;-</span> <span class="fu">mbg</span><span class="fu">::</span><span class="fu"><a href="../reference/build_id_raster.html">build_id_raster</a></span><span class="op">(</span></span>
<span>  polygons <span class="op">=</span> <span class="va">departments</span>,</span>
<span>  template_raster <span class="op">=</span> <span class="va">covariates</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="in-sample-model-comparison">In-sample model comparison<a class="anchor" aria-label="anchor" href="#in-sample-model-comparison"></a>
</h2>
<p>First, run both the standard and stacking model types using the full
dataset. These are the “in-sample” models, as no data was reserved for
validation.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Standard model (in-sample)</span></span>
<span><span class="va">standard_model_is</span> <span class="op">&lt;-</span> <span class="fu">mbg</span><span class="fu">::</span><span class="va"><a href="../reference/MbgModelRunner.html">MbgModelRunner</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span></span>
<span>  input_data <span class="op">=</span> <span class="va">outcomes</span>,</span>
<span>  id_raster <span class="op">=</span> <span class="va">id_raster</span>,</span>
<span>  covariate_rasters <span class="op">=</span> <span class="va">covariates</span>,</span>
<span>  verbose <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span><span class="va">standard_model_is</span><span class="op">$</span><span class="fu">run_mbg_pipeline</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Stacked generalization model (in-sample)</span></span>
<span><span class="co"># Same cross-validation settings</span></span>
<span><span class="va">cross_validation_settings</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">'repeatedcv'</span>, number <span class="op">=</span> <span class="fl">5</span>, repeats <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span><span class="va">submodel_settings</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>enet <span class="op">=</span> <span class="cn">NULL</span>, gbm <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>verbose <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>, treebag <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></span>
<span><span class="va">stacking_model_is</span> <span class="op">&lt;-</span> <span class="fu">mbg</span><span class="fu">::</span><span class="va"><a href="../reference/MbgModelRunner.html">MbgModelRunner</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span></span>
<span>  input_data <span class="op">=</span> <span class="va">outcomes</span>,</span>
<span>  id_raster <span class="op">=</span> <span class="va">id_raster</span>,</span>
<span>  covariate_rasters <span class="op">=</span> <span class="va">covariates</span>,</span>
<span>  use_stacking <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  stacking_cv_settings <span class="op">=</span> <span class="va">cross_validation_settings</span>,</span>
<span>  stacking_model_settings <span class="op">=</span> <span class="va">submodel_settings</span>,</span>
<span>  stacking_prediction_range <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>  stacking_use_admin_bounds <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  admin_bounds <span class="op">=</span> <span class="va">departments</span>,</span>
<span>  admin_bounds_id <span class="op">=</span> <span class="st">'department_code'</span>,</span>
<span>  verbose <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span><span class="va">stacking_model_is</span><span class="op">$</span><span class="fu">run_mbg_pipeline</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; Loading required package: ggplot2</span></span>
<span><span class="co">#&gt; Loading required package: lattice</span></span></code></pre></div>
<p>Use the <code>MbgModelRunner$get_predictive_validity()</code> method
to calculate in-sample LPD, WAIC, and RMSE for each model:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">standard_model_metrics</span> <span class="op">&lt;-</span> <span class="va">standard_model_is</span><span class="op">$</span><span class="fu">get_predictive_validity</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">standard_model_metrics</span><span class="op">$</span><span class="va">model_type</span> <span class="op">&lt;-</span> <span class="st">"Standard"</span></span>
<span><span class="va">stacking_model_metrics</span> <span class="op">&lt;-</span> <span class="va">stacking_model_is</span><span class="op">$</span><span class="fu">get_predictive_validity</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">stacking_model_metrics</span><span class="op">$</span><span class="va">model_type</span> <span class="op">&lt;-</span> <span class="st">"Stacked ensemble"</span></span>
<span></span>
<span><span class="va">metrics_in_sample</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">rbind</a></span><span class="op">(</span></span>
<span>  <span class="va">standard_model_metrics</span>,</span>
<span>  <span class="va">stacking_model_metrics</span></span>
<span><span class="op">)</span></span>
<span><span class="va">metrics_in_sample</span></span>
<span><span class="co">#&gt;      rmse_is    lpd_is  waic_is       model_type</span></span>
<span><span class="co">#&gt;        &lt;num&gt;     &lt;num&gt;    &lt;num&gt;           &lt;char&gt;</span></span>
<span><span class="co">#&gt; 1: 0.1285016 -1206.078 2397.115         Standard</span></span>
<span><span class="co">#&gt; 2: 0.1173281 -1161.453 2310.004 Stacked ensemble</span></span></code></pre></div>
<p>The stacked ensemble model has a slightly lower RMSE and a higher
(less negative) LPD, indicating a closer fit to the observed data.
However, this may just indicate a more flexible model fit—overfitting
could lead to worse model performance when comparing against new,
unobserved data.</p>
<p>We can also calculate the in-sample RMSE of each component ML model
from the stacked ensemble model:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">outcomes</span><span class="op">[</span>, <span class="va">data_rate</span> <span class="op">:=</span> <span class="va">indicator</span> <span class="op">/</span> <span class="va">samplesize</span><span class="op">]</span></span>
<span><span class="va">ml_submodels</span> <span class="op">&lt;-</span> <span class="va">stacking_model_is</span><span class="op">$</span><span class="va">model_covariates</span></span>
<span></span>
<span><span class="va">submodel_rmse</span> <span class="op">&lt;-</span> <span class="fu">data.table</span><span class="fu">::</span><span class="fu"><a href="https://rdatatable.gitlab.io/data.table/reference/data.table.html" class="external-link">data.table</a></span><span class="op">(</span></span>
<span>  rmse_is <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span></span>
<span>    <span class="fu">mbg</span><span class="fu">::</span><span class="fu"><a href="../reference/rmse_raster_to_point.html">rmse_raster_to_point</a></span><span class="op">(</span></span>
<span>      estimates <span class="op">=</span> <span class="va">ml_submodels</span><span class="op">$</span><span class="va">enet</span>,</span>
<span>      validation_data <span class="op">=</span> <span class="va">outcomes</span>,</span>
<span>      outcome_field <span class="op">=</span> <span class="st">'data_rate'</span></span>
<span>    <span class="op">)</span>,</span>
<span>    <span class="fu">mbg</span><span class="fu">::</span><span class="fu"><a href="../reference/rmse_raster_to_point.html">rmse_raster_to_point</a></span><span class="op">(</span></span>
<span>      estimates <span class="op">=</span> <span class="va">ml_submodels</span><span class="op">$</span><span class="va">gbm</span>,</span>
<span>      validation_data <span class="op">=</span> <span class="va">outcomes</span>,</span>
<span>      outcome_field <span class="op">=</span> <span class="st">'data_rate'</span></span>
<span>    <span class="op">)</span>,</span>
<span>    <span class="fu">mbg</span><span class="fu">::</span><span class="fu"><a href="../reference/rmse_raster_to_point.html">rmse_raster_to_point</a></span><span class="op">(</span></span>
<span>      estimates <span class="op">=</span> <span class="va">ml_submodels</span><span class="op">$</span><span class="va">treebag</span>,</span>
<span>      validation_data <span class="op">=</span> <span class="va">outcomes</span>,</span>
<span>      outcome_field <span class="op">=</span> <span class="st">'data_rate'</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span>,</span>
<span>  model_type <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">'Elastic net'</span>, <span class="st">'Gradient boosted machines'</span>, <span class="st">'Bagged regression trees'</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">submodel_rmse</span></span>
<span><span class="co">#&gt;      rmse_is                model_type</span></span>
<span><span class="co">#&gt;        &lt;num&gt;                    &lt;char&gt;</span></span>
<span><span class="co">#&gt; 1: 0.1337504               Elastic net</span></span>
<span><span class="co">#&gt; 2: 0.1265685 Gradient boosted machines</span></span>
<span><span class="co">#&gt; 3: 0.1161694   Bagged regression trees</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="out-of-sample-model-comparison">Out-of-sample model comparison<a class="anchor" aria-label="anchor" href="#out-of-sample-model-comparison"></a>
</h2>
<p>To better approximate model performance with unobserved data, we will
perform 10-fold validation using the input dataset. This requires
running both the standard and stacking models 10 times on different
subsets of the observed point data. For each of the 10 sub-models, we
generate predictive validity metrics based on the held-out portion of
the data.</p>
<p>Note that WAIC is an in-sample predictive validity metric: because
<code>in_sample = FALSE</code>, WAIC is not generated during these
runs.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Assign a new `holdout_id` field in the data, which are shuffled integers from 1 to 10</span></span>
<span><span class="va">n_holdouts</span> <span class="op">&lt;-</span> <span class="fl">10</span></span>
<span><span class="va">outcomes</span><span class="op">$</span><span class="va">holdout_id</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_len</a></span><span class="op">(</span><span class="va">n_holdouts</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span>length.out <span class="op">=</span> <span class="fu"><a href="https://rspatial.github.io/terra/reference/dimensions.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">outcomes</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># For each of the 10 holdouts, get both models' *out-of-sample* predictive validity</span></span>
<span><span class="va">metrics_by_holdout</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">lapply</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_len</a></span><span class="op">(</span><span class="va">n_holdouts</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">holdout</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="co"># Split into (observed) training data and (unobserved) testing data</span></span>
<span>  <span class="va">train</span> <span class="op">&lt;-</span> <span class="va">outcomes</span><span class="op">[</span><span class="va">holdout_id</span> <span class="op">!=</span> <span class="va">holdout</span>,<span class="op">]</span></span>
<span>  <span class="va">test</span> <span class="op">&lt;-</span> <span class="va">outcomes</span><span class="op">[</span><span class="va">holdout_id</span> <span class="op">==</span> <span class="va">holdout</span>,<span class="op">]</span></span>
<span>  <span class="co"># Run both models</span></span>
<span>  <span class="va">standard_model_oos</span> <span class="op">&lt;-</span> <span class="fu">mbg</span><span class="fu">::</span><span class="va"><a href="../reference/MbgModelRunner.html">MbgModelRunner</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span></span>
<span>    input_data <span class="op">=</span> <span class="va">train</span>,</span>
<span>    id_raster <span class="op">=</span> <span class="va">id_raster</span>,</span>
<span>    covariate_rasters <span class="op">=</span> <span class="va">covariates</span>,</span>
<span>    verbose <span class="op">=</span> <span class="cn">FALSE</span></span>
<span>  <span class="op">)</span></span>
<span>  <span class="va">standard_model_oos</span><span class="op">$</span><span class="fu">run_mbg_pipeline</span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="va">stacking_model_oos</span> <span class="op">&lt;-</span> <span class="fu">mbg</span><span class="fu">::</span><span class="va"><a href="../reference/MbgModelRunner.html">MbgModelRunner</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span></span>
<span>    input_data <span class="op">=</span> <span class="va">train</span>,</span>
<span>    id_raster <span class="op">=</span> <span class="va">id_raster</span>,</span>
<span>    covariate_rasters <span class="op">=</span> <span class="va">covariates</span>,</span>
<span>    use_stacking <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>    stacking_cv_settings <span class="op">=</span> <span class="va">cross_validation_settings</span>,</span>
<span>    stacking_model_settings <span class="op">=</span> <span class="va">submodel_settings</span>,</span>
<span>    stacking_prediction_range <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>    stacking_use_admin_bounds <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>    admin_bounds <span class="op">=</span> <span class="va">departments</span>,</span>
<span>    admin_bounds_id <span class="op">=</span> <span class="st">'department_code'</span>,</span>
<span>    verbose <span class="op">=</span> <span class="cn">FALSE</span></span>
<span>  <span class="op">)</span></span>
<span>  <span class="va">stacking_model_oos</span><span class="op">$</span><span class="fu">run_mbg_pipeline</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span>  <span class="co"># Compare to the test data</span></span>
<span>  <span class="va">standard_metrics</span> <span class="op">&lt;-</span> <span class="va">standard_model_oos</span><span class="op">$</span><span class="fu">get_predictive_validity</span><span class="op">(</span></span>
<span>    in_sample <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>    validation_data <span class="op">=</span> <span class="va">test</span></span>
<span>  <span class="op">)</span><span class="op">[</span>, <span class="va">model_type</span> <span class="op">:=</span> <span class="st">'Standard'</span><span class="op">]</span></span>
<span>  <span class="va">stacking_metrics</span> <span class="op">&lt;-</span> <span class="va">stacking_model_oos</span><span class="op">$</span><span class="fu">get_predictive_validity</span><span class="op">(</span></span>
<span>    in_sample <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>    validation_data <span class="op">=</span> <span class="va">test</span></span>
<span>  <span class="op">)</span><span class="op">[</span>, <span class="va">model_type</span> <span class="op">:=</span> <span class="st">'Stacked ensemble'</span><span class="op">]</span></span>
<span>  <span class="va">this_holdout_metrics</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">standard_metrics</span>, <span class="va">stacking_metrics</span><span class="op">)</span></span>
<span>  <span class="va">this_holdout_metrics</span><span class="op">$</span><span class="va">holdout_id</span> <span class="op">&lt;-</span> <span class="va">holdout</span></span>
<span></span>
<span>  <span class="co"># Return the combined metrics for this holdout</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="va">this_holdout_metrics</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">data.table</span><span class="fu">::</span><span class="fu"><a href="https://rdatatable.gitlab.io/data.table/reference/rbindlist.html" class="external-link">rbindlist</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<table class="table">
<thead><tr class="header">
<th align="right">rmse_oos</th>
<th align="right">lpd_oos</th>
<th align="left">model_type</th>
<th align="right">holdout_id</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">0.1181227</td>
<td align="right">-118.7762</td>
<td align="left">Standard</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">0.1252693</td>
<td align="right">-120.7670</td>
<td align="left">Stacked ensemble</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">0.1290111</td>
<td align="right">-120.6368</td>
<td align="left">Standard</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="right">0.1192613</td>
<td align="right">-117.7395</td>
<td align="left">Stacked ensemble</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="right">0.1497597</td>
<td align="right">-126.6321</td>
<td align="left">Standard</td>
<td align="right">3</td>
</tr>
<tr class="even">
<td align="right">0.1503554</td>
<td align="right">-128.1359</td>
<td align="left">Stacked ensemble</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td align="right">0.1413851</td>
<td align="right">-125.7261</td>
<td align="left">Standard</td>
<td align="right">4</td>
</tr>
<tr class="even">
<td align="right">0.1419081</td>
<td align="right">-128.7285</td>
<td align="left">Stacked ensemble</td>
<td align="right">4</td>
</tr>
<tr class="odd">
<td align="right">0.1360917</td>
<td align="right">-131.3189</td>
<td align="left">Standard</td>
<td align="right">5</td>
</tr>
<tr class="even">
<td align="right">0.1387924</td>
<td align="right">-136.8416</td>
<td align="left">Stacked ensemble</td>
<td align="right">5</td>
</tr>
<tr class="odd">
<td align="right">0.1392353</td>
<td align="right">-123.4548</td>
<td align="left">Standard</td>
<td align="right">6</td>
</tr>
<tr class="even">
<td align="right">0.1344768</td>
<td align="right">-121.3066</td>
<td align="left">Stacked ensemble</td>
<td align="right">6</td>
</tr>
<tr class="odd">
<td align="right">0.1386249</td>
<td align="right">-118.8217</td>
<td align="left">Standard</td>
<td align="right">7</td>
</tr>
<tr class="even">
<td align="right">0.1415001</td>
<td align="right">-121.4032</td>
<td align="left">Stacked ensemble</td>
<td align="right">7</td>
</tr>
<tr class="odd">
<td align="right">0.1255616</td>
<td align="right">-122.7545</td>
<td align="left">Standard</td>
<td align="right">8</td>
</tr>
<tr class="even">
<td align="right">0.1267633</td>
<td align="right">-124.6432</td>
<td align="left">Stacked ensemble</td>
<td align="right">8</td>
</tr>
<tr class="odd">
<td align="right">0.1463734</td>
<td align="right">-126.9334</td>
<td align="left">Standard</td>
<td align="right">9</td>
</tr>
<tr class="even">
<td align="right">0.1383586</td>
<td align="right">-124.1185</td>
<td align="left">Stacked ensemble</td>
<td align="right">9</td>
</tr>
<tr class="odd">
<td align="right">0.1378106</td>
<td align="right">-123.0726</td>
<td align="left">Standard</td>
<td align="right">10</td>
</tr>
<tr class="even">
<td align="right">0.1484380</td>
<td align="right">-129.2247</td>
<td align="left">Stacked ensemble</td>
<td align="right">10</td>
</tr>
</tbody>
</table>
<p>The models trade off for better performance across the ten holdout
folds. To get an overall comparison of out-of-sample predictive
validity, we combine these metrics across holdouts:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">out_of_sample_overall</span> <span class="op">&lt;-</span> <span class="va">metrics_by_holdout</span><span class="op">[</span></span>
<span>  , <span class="fu">.</span><span class="op">(</span>rmse_oos <span class="op">=</span> <span class="fu"><a href="https://rspatial.github.io/terra/reference/summarize-generics.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">rmse_oos</span><span class="op">)</span>, lpd_oos <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">lpd_oos</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  by <span class="op">=</span> <span class="va">model_type</span></span>
<span><span class="op">]</span></span></code></pre></div>
<table class="table">
<thead><tr class="header">
<th align="left">model_type</th>
<th align="right">rmse_oos</th>
<th align="right">lpd_oos</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Standard</td>
<td align="right">0.1361976</td>
<td align="right">-1238.127</td>
</tr>
<tr class="even">
<td align="left">Stacked ensemble</td>
<td align="right">0.1365123</td>
<td align="right">-1252.909</td>
</tr>
</tbody>
</table>
<p>In this case, the standard model has better out-of-sample LPD and
RMSE, suggesting that it is the superior model for predicting new child
stunting observations in Benin.</p>
</div>
<div class="section level2">
<h2 id="summary">Summary<a class="anchor" aria-label="anchor" href="#summary"></a>
</h2>
<p>We can generalize this tutorial into a general strategy for comparing
predictive models in a particular country/outcome/dataset context:</p>
<ol style="list-style-type: decimal">
<li>Select the universe of model types to test</li>
<li>Before running any models, select the criteria for model selection.
We recommend out-of-sample LPD as the primary criterion for model
selection, with out-of-sample RMSE as a tie-breaker for models with
near-identical LPD.</li>
<li>Split the data into <em>k</em> holdouts. Run a model for each
holdout, evaluating the subset model against the held-out data.
<em>Model comparisons should always be run against the same outcomes
dataset and holdouts.</em>
</li>
<li>Based on the criteria selected in (2), choose the model with the top
performance</li>
<li>Run a full model with this same specification using all observations
(no holdouts). These are the best results from this comparison
round.</li>
</ol>
<div class="section level3">
<h3 id="further-reading">Further reading<a class="anchor" aria-label="anchor" href="#further-reading"></a>
</h3>
<p>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2017). Model
assessment and selection. <em>The elements of statistical learning: data
mining, inference, and prediction (2E)</em>, 219-259. <a href="https://hastie.su.domains/ElemStatLearn/" class="external-link uri">https://hastie.su.domains/ElemStatLearn/</a></p>
<p>Vehtari, A., Gelman, A., &amp; Gabry, J. (2017). Practical Bayesian
model evaluation using leave-one-out cross-validation and WAIC.
<em>Statistics and computing</em>, 27, 1413-1432. <a href="https://link.springer.com/article/10.1007/s11222-016-9696-4" class="external-link uri">https://link.springer.com/article/10.1007/s11222-016-9696-4</a></p>
<p>Watanabe, S. (2010). Asymptotic equivalence of Bayes cross validation
and widely applicable information criterion in singular learning theory.
<em>Journal of machine learning research</em>, 11(12). <a href="https://www.jmlr.org/papers/volume11/watanabe10a/watanabe10a.pdf" class="external-link uri">https://www.jmlr.org/papers/volume11/watanabe10a/watanabe10a.pdf</a></p>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Nathaniel Henry, Benjamin Mayala.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
